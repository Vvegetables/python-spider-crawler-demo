#coding=utf-8
import socket
from urlparse import urljoin

import chardet
from lxml import etree
import queue
import requests

from education_crawler.utils.self_excel import excel_read, get_excel_cell_data


class SelfProcess:
    def __init__(self,_start_url,deep = 1,path = None,timeout=5):
        self.url = ((u for u in excel_read(path)) if path else None) or _start_url
        self.deep = deep
        self.data_q = queue.Queue()
        self.url_q = queue.Queue()
        self.spided_urls = None
        self.crawled_items = []
        self.sheettitle = []
    
        
    def spider(self):
        if not isinstance(self.url,(list,tuple)):
            self.url = [self.url]
        for k,_url in enumerate(self.url,2):
            source = get_excel_cell_data(_url,k,2)
            e_url = get_excel_cell_data(_url,k,3)
            if e_url:
                self.url_q.put(e_url)
            deep = 0
            while not self.url_q.empty() and deep < self.deep:
                c_url = self.url_q.get()
                if c_url in self.spided_urls:
                    continue
                _response = self.handle_url(c_url)
                _html = self.get_html(_response)
                self.get_data(_html,_url)
                for item in self.crawled_items:
                    item.append(source)
                    self.url_q.put(item[1])
                    self.data_q.put(item)
                self.crawled_items = []
                
    
    def handle_url(self,url,method='GET',timeout=5,*args,**kwargs):
        r = {
            'GET': requests.get,
            'POST': requests.post,
        }
        try:
            response = r.get(method)(url, timeout=timeout, *args, **kwargs)
            if 'Connection' in response.headers.keys():
                del response.headers['Connection']
        except socket.timeout:
            print 'timeout'
            response = self.err_response()
        except:
            response = self.err_response()
        response = self.html_code(response)
        return response
    
    def err_response(self):
        class Response:
            text = ''
            error = True
            url = self.url

            def __repr__(self):
                return 'Bad requests'
        return Response
    
    def html_code(self, response):
        if hasattr(response, 'error'):
            return response
        cD = chardet.detect(response.content)
        CHARSET = 'utf8'
        charset = cD.get('encoding', CHARSET)
        response.encoding = charset
        return response
    
    def get_html(self,response):
        if hasattr(response,'error'):
            return None
        text = response.text.replace('?xml','head')
        return etree.HTML(text)
    
    def get_data(self,html,main_url):
        if not html:
            return
        infos = html.xpath('//a')
        for info in infos:
            url = ''.join(info.xpath('@href'))
            abs_url = urljoin(main_url, url)
            if abs_url not in self.spided_urls:
                title = info.xpath('string(.)')
                self.crawled_items.append([title,abs_url])
                self.spided_urls.append(abs_url)
        